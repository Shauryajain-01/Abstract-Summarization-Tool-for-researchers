# -*- coding: utf-8 -*-
"""MILESTONE_PROJECT_2:SkimLit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14LiAlVgSBNs2MHEehxkYT_TCFGWjru-p

258
#Milestone Project 2: SkimLit
"""

#Get data

!git clone https://github.com/Franck-Dernoncourt/pubmed-rct
!ls pubmed-rct

!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/

#STarting with 20k dataset with nos replaced with @
data_dir="/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"

import os
filenames=[data_dir + filename for filename in os.listdir(data_dir)]
filenames

"""259
#Visualising examples from the dataset
"""

def get_lines(filename):
  with open(filename, "r") as f:
    return f.readlines()

train_lines = get_lines(data_dir+"train.txt")
train_lines[:29]

len(train_lines)

#Let's think about how we want our data to look..

"""260
#Writing a preprocessing function

"""

def preprocess_text_with_line_numbers(filename):
  input_lines=get_lines(filename)
  abstract_lines=""
  abstract_samples=[]

  for line in input_lines:
    if line.startswith("###"):
      abstract_id=line
      abstract_lines=""
    elif line.isspace():
        abstract_line_split=abstract_lines.splitlines()
        for abstract_line_number, abstract_line in enumerate(abstract_line_split):
             line_data = {} # create empty dict to store data from line
             target_text_split = abstract_line.split("\t") # split target label from text
             line_data["target"] = target_text_split[0] # get target label
             line_data["text"] = target_text_split[1].lower() # get target text and lower it
             line_data["line_number"] = abstract_line_number # what number line does the line appear in the abstract?
             line_data["total_lines"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)
             abstract_samples.append(line_data) # add line data to abstract samples list

    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence
      abstract_lines += line

  return abstract_samples

train_samples=preprocess_text_with_line_numbers(data_dir+"train.txt")
val_samples = preprocess_text_with_line_numbers(data_dir + "dev.txt") # dev is another name for validation set
test_samples = preprocess_text_with_line_numbers(data_dir + "test.txt")
len(train_samples), len(val_samples), len(test_samples)

# Check the first abstract of our training data
train_samples[:14]

"""261"""

import pandas as pd
train_df=pd.DataFrame(train_samples)
val_df = pd.DataFrame(val_samples)
test_df = pd.DataFrame(test_samples)
train_df.head(14)

train_df.target.value_counts()

train_df.total_lines.plot.hist();

# Convert abstract text lines into lists
train_sentences = train_df["text"].tolist()
val_sentences = val_df["text"].tolist()
test_sentences = test_df["text"].tolist()
len(train_sentences), len(val_sentences), len(test_sentences)

train_sentences[:10]

"""262
#Turning our target labels into numbers
"""

#one hot encoding labels
from sklearn.preprocessing import OneHotEncoder
one_hot_encoder = OneHotEncoder(sparse=False)
train_labels_one_hot = one_hot_encoder.fit_transform(train_df["target"].to_numpy().reshape(-1, 1))
val_labels_one_hot = one_hot_encoder.transform(val_df["target"].to_numpy().reshape(-1, 1))
test_labels_one_hot = one_hot_encoder.transform(test_df["target"].to_numpy().reshape(-1, 1))

# Check what training labels look like
train_labels_one_hot

"""Label Encode labels"""

# Extract labels ("target" columns) and encode them into integers
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_df["target"].to_numpy())
val_labels_encoded = label_encoder.transform(val_df["target"].to_numpy())
test_labels_encoded = label_encoder.transform(test_df["target"].to_numpy())

# Check what training labels look like
train_labels_encoded[:10]

#Get class
num_classes = len(label_encoder.classes_)
class_names = label_encoder.classes_
num_classes, class_names

"""263

#Model 0: Getting baseline (TF-IDF Multinomial Naive Bayes classifier)
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

#create pipeline
model_0=Pipeline([
    ("tf-idf", TfidfVectorizer()),
    ("clf",MultinomialNB())
])

#Fit the pipeline
model_0.fit(X=train_sentences,
            y=train_labels_encoded)

#evaluate
model_0.score(X=val_sentences,
              y=val_labels_encoded)

baseline_preds=model_0.predict(val_sentences)
baseline_preds

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

from helper_functions import calculate_results

baseline_results=calculate_results(y_true=val_labels_encoded,
                                   y_pred=baseline_preds)
baseline_results

"""264
#Preprocess for deep sequence models
"""

#Preparing our data (text) for deep sequence models
import numpy as np
import tensorflow as tf
from tensorflow .keras import layers

# How long is each sentence on average?
sent_lens = [len(sentence.split()) for sentence in train_sentences]
avg_sent_len = np.mean(sent_lens)
avg_sent_len

# What's the distribution look like?
import matplotlib.pyplot as plt
plt.hist(sent_lens, bins=20);

# How long of a sentence covers 95% of the lengths?
output_seq_len = int(np.percentile(sent_lens, 95))
output_seq_len

#maximum deq len in training set
max(sent_lens)

"""265
#creating a text vectorizer
"""

#words in vocab
max_tokens=68000

#create a text vectorizer
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

text_vectorizer=TextVectorization(max_tokens=max_tokens, # number of words in vocabulary
                                    output_sequence_length=55) # desired output length of vectorized sequences

# Adapt text vectorizer to training sentences
text_vectorizer.adapt(train_sentences)

# Test out text vectorizer
import random
target_sentence = random.choice(train_sentences)
print(f"Text:\n{target_sentence}")
print(f"\nLength of text: {len(target_sentence.split())}")
print(f"\nVectorized text:\n{text_vectorizer([target_sentence])}")

rct_20k_text_vocab=text_vectorizer.get_vocabulary()
print(f"Number of words in vocabulary: {len(rct_20k_text_vocab)}"),
print(f"Most common words in the vocabulary: {rct_20k_text_vocab[:5]}")
print(f"Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}")

text_vectorizer.get_config()

"""266
#Creating embedding layer
"""

token_embed=layers.Embedding=layers.Embedding(input_dim=len(rct_20k_text_vocab),
                                              output_dim=128,
                                              mask_zero=True,
                                              name="token_embedding"
                                          )

print(target_sentence)

vectorized_sentence=text_vectorizer([target_sentence])

vectorized_sentence

embedded_sentence=token_embed(vectorized_sentence)

embedded_sentence

embedded_sentence.shape

"""267
#Creating datasets
"""

# Turn our data into TensorFlow Datasets
train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))
valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))
test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))

train_dataset

# Take the TensorSliceDataset's and turn them into prefetched batches
train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)

train_dataset

"""268
#Model 1: Conv1D
"""

# Create 1D convolutional model to process sequences
inputs = layers.Input(shape=(1,), dtype=tf.string)
text_vectors = text_vectorizer(inputs) # vectorize text inputs
token_embeddings = token_embed(text_vectors) # create embedding
x = layers.Conv1D(64, kernel_size=5, padding="same", activation="relu")(token_embeddings)
x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector
outputs = layers.Dense(num_classes, activation="softmax")(x)
model_1 = tf.keras.Model(inputs, outputs)

# Compile
model_1.compile(loss="categorical_crossentropy", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_1.summary()

len(train_dataset)

# Fit the model
model_1_history = model_1.fit(train_dataset,
                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time
                              epochs=3,
                              validation_data=valid_dataset,
                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10%

model_1.evaluate(valid_dataset)

model_1_pred_probs=model_1.predict(valid_dataset)
model_1_pred_probs

model_1_preds=tf.argmax(model_1_pred_probs, axis=1)
model_1_preds

model_1_results=calculate_results(y_true=val_labels_encoded,
                                  y_pred=model_1_preds)
model_1_results

baseline_results

"""269
#Model 2: Feature Extraction with Pretrained token embeddings
"""

# Download pretrained TensorFlow Hub USE
import tensorflow_hub as hub
tf_hub_embedding_layer = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4",
                                        trainable=False,
                                        name="universal_sentence_encoder")

# Test out the embedding on a random sentence
random_training_sentence = random.choice(train_sentences)
print(f"Random training sentence:\n{random_training_sentence}\n")
use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence])
print(f"Sentence after embedding:\n{use_embedded_sentence[0][:30]} (truncated output)...\n")
print(f"Length of sentence embedding:\n{len(use_embedded_sentence[0])}")

"""270"""

# Define feature extractor model using TF Hub layer
inputs = layers.Input(shape=[], dtype=tf.string)
pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding
x = layers.Dense(128, activation="relu")(pretrained_embedding) # add a fully connected layer on top of the embedding
# Note: you could add more layers here if you wanted to
outputs = layers.Dense(5, activation="softmax")(x) # create the output layer
model_2 = tf.keras.Model(inputs=inputs,
                        outputs=outputs)

# Compile the model
model_2.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_2.summary()

# Fit feature extractor model for 3 epochs
model_2.fit(train_dataset,
            steps_per_epoch=int(0.1 * len(train_dataset)),
            epochs=3,
            validation_data=valid_dataset,
            validation_steps=int(0.1 * len(valid_dataset)))

# Evaluate on whole validation dataset
model_2.evaluate(valid_dataset)

# Make predictions with feature extraction model
model_2_pred_probs = model_2.predict(valid_dataset)
model_2_pred_probs

# Convert the predictions with feature extraction model to classes
model_2_preds = tf.argmax(model_2_pred_probs, axis=1)
model_2_preds

# Calculate results from TF Hub pretrained embeddings results on validation set
model_2_results = calculate_results(y_true=val_labels_encoded,
                                    y_pred=model_2_preds)
model_2_results

model_1_results

baseline_results

"""271
#Model 3: Conv1D with character embeddings

#each character turns into a feature vector
"""

train_sentences[:5]

def split_chars(text):
  return " ".join(list(text))

" ".join(list(train_sentences[0]))

split_chars(random_training_sentence)

train_chars = [split_chars(sentence) for sentence in train_sentences]
val_chars = [split_chars(sentence) for sentence in val_sentences]
test_chars = [split_chars(sentence) for sentence in test_sentences]
print(train_chars[:5])

#what is the average character length
char_lens = [len(sentence) for sentence in train_sentences]
mean_char_lens = np.mean(char_lens)
mean_char_lens

#distribution
import matplotlib.pyplot as plt
plt.hist(char_lens, bins = 7)

#finding the  char length which covers 95 percent
output_seq_char_len = int(np.percentile(char_lens, 95))
output_seq_char_len

#get all keyboard character
random.choice(train_sentences)

import string
alphabet = string.ascii_lowercase + string.digits + string.punctuation
alphabet

NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token (OOV = out of vocab, '[UNK]')
char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,
                                    output_sequence_length=output_seq_char_len,
                                     standardize="lower_and_strip_punctuation",
                                    name="char_vectorizer")

#Adapt character vectorizer to training character
char_vectorizer.adapt(train_chars)

char_vocab = char_vectorizer.get_vocabulary()
print("Number of different charecters in character vocab: ", len(char_vocab))
print("5 most common character: ", char_vocab[:5])
print("5 least common characte: ", char_vocab[-5])

random_train_chars = random.choice(train_chars)
print(f"Charified text:\n {random_train_chars}")
print(f"\nLength of random_train_chars: {len(random_train_chars.split())}")
vectorized_chars = char_vectorizer([random_train_chars])
print(f"\nVectorized chars:\n {vectorized_chars}")
print(f"\nLength of vectorized chars: {len(vectorized_chars[0])}")

"""272"""

char_embed = layers.Embedding(input_dim = NUM_CHAR_TOKENS,
                              output_dim = 25,
                              mask_zero = True,
                              name = "char_embed")







"""273
#Model 3: Conv1D model with character embeddings
"""

